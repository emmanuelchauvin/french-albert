# French-ALBERT
We built a french ALBERT model.

ALBERT is "A Lite" version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation.

For a technical description of the algorithm, see the paper:  
[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

The performance of ALBERT on GLUE benchmark are showed on Github from this link : [results](https://github.com/google-research/albert#results) 

From a complete Wikipedia French Dataset we built a ALBERT Base model.


Thank you to [TensorFlow Research Cloud (TFRC)](https://www.tensorflow.org/tfrc) for their support and help during this project.
